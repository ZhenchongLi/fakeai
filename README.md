# OpenResty OpenAI API æ¨¡æ‹Ÿå™¨

ä¸€ä¸ªé«˜æ€§èƒ½çš„OpenResty Luaè„šæœ¬ï¼Œç”¨äºæ¨¡æ‹ŸOpenAI APIçš„æµå¼å“åº”ã€‚æ— è®ºå®¢æˆ·ç«¯å‘é€ä»€ä¹ˆå†…å®¹ï¼Œéƒ½ä¼šå¿«é€Ÿè¿”å›"powered by openai"çš„æµå¼å“åº”ã€‚

## ç‰¹æ€§

- ğŸš€ é«˜æ€§èƒ½ï¼šåŸºäºOpenRestyå’ŒLuaï¼Œæ”¯æŒé«˜å¹¶å‘
- ğŸ“¡ æµå¼å“åº”ï¼šå®Œå…¨æ¨¡æ‹ŸOpenAI APIçš„Server-Sent Eventsæ ¼å¼
- ğŸ”„ å³æ—¶å“åº”ï¼šä¸ç®¡è¾“å…¥ä»€ä¹ˆå†…å®¹ï¼Œç«‹å³å¼€å§‹æµå¼è¿”å›
- ğŸ¯ å…¼å®¹æ€§ï¼šå®Œå…¨å…¼å®¹OpenAI APIæ ¼å¼
- âš¡ è½»é‡çº§ï¼šæœ€å°åŒ–ä¾èµ–ï¼Œå¿«é€Ÿéƒ¨ç½²

## æ–‡ä»¶ç»“æ„

```
.
â”œâ”€â”€ openai_stream_mock.lua  # ä¸»è¦çš„Luaè„šæœ¬
â”œâ”€â”€ nginx.conf             # OpenRestyé…ç½®æ–‡ä»¶
â”œâ”€â”€ test_client.py         # Pythonæµ‹è¯•å®¢æˆ·ç«¯
â”œâ”€â”€ start.sh              # å¯åŠ¨è„šæœ¬
â””â”€â”€ README.md             # è¯´æ˜æ–‡æ¡£
```

## å®‰è£…è¦æ±‚

- OpenResty (åŒ…å«Nginx + Lua)
- Python 3 (ç”¨äºæµ‹è¯•å®¢æˆ·ç«¯)

### å®‰è£…OpenResty

**macOS:**
```bash
brew install openresty
```

**Ubuntu/Debian:**
```bash
sudo apt-get update
sudo apt-get install openresty
```

**CentOS/RHEL:**
```bash
sudo yum install openresty
```

## ä½¿ç”¨æ–¹æ³•

### 1. å¯åŠ¨æœåŠ¡

```bash
./start.sh
```

æœåŠ¡å°†åœ¨ `http://localhost:8080` å¯åŠ¨

### 2. æµ‹è¯•API

ä½¿ç”¨Pythonæµ‹è¯•å®¢æˆ·ç«¯ï¼š
```bash
python3 test_client.py
```

æˆ–ä½¿ç”¨curlï¼š
```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"gpt-3.5-turbo","messages":[{"role":"user","content":"Hello"}],"stream":true}'
```

### 3. åœæ­¢æœåŠ¡

```bash
openresty -p $(pwd) -s stop
```

## APIç«¯ç‚¹

- `POST /v1/chat/completions` - æ¨¡æ‹ŸOpenAIèŠå¤©å®ŒæˆAPI
- `GET /health` - å¥åº·æ£€æŸ¥ç«¯ç‚¹

## å“åº”æ ¼å¼

APIè¿”å›æ ‡å‡†çš„OpenAIæµå¼å“åº”æ ¼å¼ï¼š

```
data: {"id":"chatcmpl-123456","object":"chat.completion.chunk","created":1234567890,"model":"gpt-3.5-turbo","choices":[{"index":0,"delta":{"role":"assistant"},"finish_reason":null}]}

data: {"id":"chatcmpl-123456","object":"chat.completion.chunk","created":1234567890,"model":"gpt-3.5-turbo","choices":[{"index":0,"delta":{"content":"powered "},"finish_reason":null}]}

data: {"id":"chatcmpl-123456","object":"chat.completion.chunk","created":1234567890,"model":"gpt-3.5-turbo","choices":[{"index":0,"delta":{"content":"by "},"finish_reason":null}]}

data: {"id":"chatcmpl-123456","object":"chat.completion.chunk","created":1234567890,"model":"gpt-3.5-turbo","choices":[{"index":0,"delta":{"content":"openai"},"finish_reason":null}]}

data: {"id":"chatcmpl-123456","object":"chat.completion.chunk","created":1234567890,"model":"gpt-3.5-turbo","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

## æ€§èƒ½ä¼˜åŒ–

- å¯ç”¨äº†Luaä»£ç ç¼“å­˜ (`lua_code_cache on`)
- ä½¿ç”¨å¼‚æ­¥I/Oå’Œåç¨‹
- æœ€å°åŒ–å†…å­˜åˆ†é…
- æ”¯æŒé«˜å¹¶å‘è¿æ¥

## è‡ªå®šä¹‰é…ç½®

### ä¿®æ”¹å“åº”å†…å®¹

ç¼–è¾‘ `openai_stream_mock.lua` ä¸­çš„ `response_text` å˜é‡ï¼š

```lua
local response_text = "your custom response here"
```

### è°ƒæ•´æµå¼å»¶è¿Ÿ

ä¿®æ”¹ `ngx.sleep(0.05)` ä¸­çš„æ•°å€¼æ¥è°ƒæ•´æ¯ä¸ªè¯ä¹‹é—´çš„å»¶è¿Ÿã€‚

### æ›´æ”¹ç«¯å£

ç¼–è¾‘ `nginx.conf` ä¸­çš„ `listen` æŒ‡ä»¤ï¼š

```nginx
listen 9000;  # æ”¹ä¸ºä½ æƒ³è¦çš„ç«¯å£
```

## æ•…éšœæ’é™¤

1. **ç«¯å£è¢«å ç”¨**: ä¿®æ”¹nginx.confä¸­çš„ç«¯å£å·
2. **OpenRestyæœªæ‰¾åˆ°**: ç¡®ä¿OpenRestyå·²æ­£ç¡®å®‰è£…å¹¶åœ¨PATHä¸­
3. **æƒé™é—®é¢˜**: ç¡®ä¿è„šæœ¬æœ‰æ‰§è¡Œæƒé™ (`chmod +x start.sh`)

## è®¸å¯è¯

MIT License